{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document search1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaZHVRTwKFzCWp/uz6XK9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpk-a7/Information-Retrieval-IR-/blob/main/Document_search1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twEYJ3ehfq1R"
      },
      "source": [
        "https://iq.opengenus.org/document-similarity-tf-idf/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WGgH0ilhZ7o",
        "outputId": "4acf21a5-723c-4253-b78c-504ed371f2ae"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import math\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 43.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85452 sha256=3529cc4f4a9d1eb2c7015f82ca2916649aa9cd7eeafda98f0daa81a3d8cd7b13\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRynqM3Fdnr_"
      },
      "source": [
        "Sentence ->  Decontractor ->  Remove Punctuations -> Tokenization & Extra Spaces -> Normalization(Lemmatization or Stemming) -> Cpython(TF-IDF) !-> Term-based Similarity measuer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H5vLTtYdjR2"
      },
      "source": [
        "# Sentence\n",
        "X = [\"HC:Unsafe condition: Access not provided in the excavation for ingress/egress\",\n",
        "     \"I am interested in this meet\"][1]\n",
        "Y = [\"HC:Unsafe condition: Excavation access not provided\",\n",
        "     \"HC:Unsafe environment: No access for is provided in the excavation\",\n",
        "     \"HC:Unsafe condition: Entry and exit is not provided\",\n",
        "     \"HC:Unsafe condition: No access for ingress/egress is provided in the excavation\",\n",
        "     \"I not at all interested in this meet\",\n",
        "     \"I am kind of interested in this meet\"][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znvjJsxlVjrU",
        "outputId": "e20e3a86-1532-491c-b8db-cb1f127328ef"
      },
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "print(similar(X,Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSpuQ30ffubv"
      },
      "source": [
        "# Decontraction solution 1\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n",
        "    phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"an\\'t\", \"am not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liLl4Vwvg4GS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f46abb8-681b-4c86-e2ac-ae9bfc3bb8af"
      },
      "source": [
        "# Decontraction solution 2\n",
        "def deconstration(sentS:str)-> \"string\":\n",
        "  # creating an empty list\n",
        "  expanded_words = []    \n",
        "  for word in sentS.split(\" \"):\n",
        "    expanded_words.append(contractions.fix(word))       \n",
        "  return ' '.join(expanded_words)\n",
        "X = deconstration(X)\n",
        "Y = deconstration(Y)\n",
        "print(f\"X decontracted: {X} \\nY decontracted: {Y}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X decontracted: I am interested in this meet \n",
            "Y decontracted: I am kind of interested in this meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUL4dfIIil6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5566784-84c4-42e1-c40b-3d9b749d470a"
      },
      "source": [
        "X = re.sub(r'[^\\w\\s]', '', X)\n",
        "Y = re.sub(r'[^\\w\\s]', '', Y)\n",
        "print(f\"X removed Punctuations: {X} \\nY removed Punctuations: {Y}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X removed Punctuations: I am interested in this meet \n",
            "Y removed Punctuations: I am kind of interested in this meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u85SbWmwhykH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6014d4d3-f437-47be-a9f2-c33e98d8c729"
      },
      "source": [
        "# Tokenization and  Extra Spaces remove\n",
        "X_t = word_tokenize(X)\n",
        "Y_t = word_tokenize(Y)\n",
        "print(f\"X tokenized: {X_t} \\nY tokenized: {Y_t}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tokenized: ['I', 'am', 'interested', 'in', 'this', 'meet'] \n",
            "Y tokenized: ['I', 'am', 'kind', 'of', 'interested', 'in', 'this', 'meet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMIa5jQnipJk"
      },
      "source": [
        "# Normalization(Lemmatization or Stemming)\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "englishStemmer=SnowballStemmer(\"english\")\n",
        "\n",
        "port_X  = lambda i: [porter.stem(w) for w in i]\n",
        "lanc_X  = lambda i: [lancaster.stem(w) for w in i]\n",
        "snow_X  = lambda i: [englishStemmer.stem(w) for w in i]\n",
        "lemma_X = lambda i: [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFUrhypWizIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50d9a47-179f-4cc3-e149-cfd97ee5d5d1"
      },
      "source": [
        "print(\"PorterStemmer: \", port_X(X_t))\n",
        "print(\"LancasterStemmer: \", lanc_X(X_t)) #<- fast and simple\n",
        "print(\"SnowballStemmer: \", snow_X(X_t))\n",
        "print(\"WordNetLemmatizer:\", lemma_X(X_t)) #<- slow and efficient"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer:  ['I', 'am', 'interest', 'in', 'thi', 'meet']\n",
            "LancasterStemmer:  ['i', 'am', 'interest', 'in', 'thi', 'meet']\n",
            "SnowballStemmer:  ['i', 'am', 'interest', 'in', 'this', 'meet']\n",
            "WordNetLemmatizer: ['I', 'be', 'interest', 'in', 'this', 'meet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XhekF0wjTSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629d260e-8aa5-4650-dada-52a136b33052"
      },
      "source": [
        "X_norm = lanc_X(X_t)\n",
        "Y_norm = lanc_X(Y_t)\n",
        "print(f\"X Normalized: {X_norm} \\nY Normalized: {Y_norm}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Normalized: ['i', 'am', 'interest', 'in', 'thi', 'meet'] \n",
            "Y Normalized: ['i', 'am', 'kind', 'of', 'interest', 'in', 'thi', 'meet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i81l9XmdLMzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38548431-098c-4e07-a76e-5dbf72b76893"
      },
      "source": [
        "countvectorizer = CountVectorizer(analyzer= 'word')\n",
        "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
        "\n",
        "train = [\" \".join(X_norm), \" \".join(Y_norm)]\n",
        "count_wm = countvectorizer.fit_transform(train)\n",
        "tfidf_wm = tfidfvectorizer.fit_transform(train)\n",
        "\n",
        "count_tokens = countvectorizer.get_feature_names()\n",
        "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
        "\n",
        "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n",
        "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n",
        "print(\"Count Vectorizer\\n\")\n",
        "print(df_countvect)\n",
        "print(\"\\nTD-IDF Vectorizer\\n\")\n",
        "print(df_tfidfvect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer\n",
            "\n",
            "      am  in  interest  kind  meet  of  thi\n",
            "Doc1   1   1         1     0     1   0    1\n",
            "Doc2   1   1         1     1     1   1    1\n",
            "\n",
            "TD-IDF Vectorizer\n",
            "\n",
            "            am        in  interest      kind      meet        of       thi\n",
            "Doc1  0.447214  0.447214  0.447214  0.000000  0.447214  0.000000  0.447214\n",
            "Doc2  0.334251  0.334251  0.334251  0.469778  0.334251  0.469778  0.334251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y17QIq1Ed6oZ"
      },
      "source": [
        "# Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoAe1NYZr33k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82175f5f-0c61-4d39-c8b7-412d1055d39d"
      },
      "source": [
        "l1 = tfidf_wm.toarray()[0]\n",
        "l2 = tfidf_wm.toarray()[1]\n",
        "c = 0\n",
        "for i_1 in range(len(tfidf_tokens)):\n",
        "  c += l1[i_1] * l2[i_1]\n",
        "cosine = c / float((sum(l1) * sum(l2)) ** 0.5)\n",
        "print(\"Cosine using tfidf\",cosine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine using tfidf 0.3093337485597601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYbUAywQHujg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3448c683-a13d-4c64-e0f0-27ff1537313e"
      },
      "source": [
        "l1 = count_wm.toarray()[0]\n",
        "l2 = count_wm.toarray()[1]\n",
        "c = 0\n",
        "for i_1 in range(len(tfidf_tokens)):\n",
        "  c += l1[i_1] * l2[i_1]\n",
        "cosine = c / float((sum(l1) * sum(l2)) ** 0.5)\n",
        "print(\"Cosine using count vectorizer\",cosine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine using count vectorizer 0.8451542547285166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvleiZemd-eq"
      },
      "source": [
        "# Jaccard Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5DhRG6bPGua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a61011-bb99-45b8-cc2f-07fe4aa43432"
      },
      "source": [
        " # Jaccard Similarity(d1, d2) = d1 ∩ d2 / d1 ∪ d2\n",
        "JD = len(set(tfidf_wm.toarray()[0]).intersection(set(tfidf_wm.toarray()[1])))/len(set(tfidf_wm.toarray()[0]).union(set(tfidf_wm.toarray()[1])))\n",
        "print(\"Jaccard similarity using TFIDF: \",JD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity using TFIDF:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFBIEtaKhHPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c7e79d-a215-43e9-bed0-2f32b41de49f"
      },
      "source": [
        "def jaccard_binary(x,y):\n",
        "    \"\"\"A function for finding the similarity between two binary vectors\"\"\"\n",
        "    intersection = np.logical_and(x, y)\n",
        "    union = np.logical_or(x, y)\n",
        "    similarity = intersection.sum() / float(union.sum())\n",
        "    return similarity\n",
        "print(\"Jaccard similarity using TFIDF: \", jaccard_binary(tfidf_wm.toarray()[0],tfidf_wm.toarray()[1]))\n",
        "print(\"Jaccard similarity using Count vectorizer: \", jaccard_binary(count_wm.toarray()[0],count_wm.toarray()[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity using TFIDF:  0.7142857142857143\n",
            "Jaccard similarity using Count vectorizer:  0.7142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIJtqVjEenmh"
      },
      "source": [
        "# Euclidean Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD6x50yleUQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b60599-1a0e-4b3f-ce4f-479a99b23ed4"
      },
      "source": [
        "# Euclidean Distance = sqrt(∑(xi−yi)^2), where i = 1 to i = n (number of vectors)\n",
        "dist1 = np.linalg.norm(tfidf_wm.toarray()[0] - tfidf_wm.toarray()[1])\n",
        "print(\"Euclidean Distance using TFIDF:\",dist1)\n",
        "dist2 = np.linalg.norm(count_wm.toarray()[0] - count_wm.toarray()[1])\n",
        "print(\"Euclidean Distance using Count vectorizer:\",dist2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance using TFIDF: 0.7107638792087759\n",
            "Euclidean Distance using Count vectorizer: 1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQRycGdXfBXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167e1c1b-84e0-4b1c-cee7-9033bcc3ea5b"
      },
      "source": [
        "\n",
        "bool(dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw-W0OBerFD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eea80a3-7838-4a12-fc1d-3744da5f67f0"
      },
      "source": [
        "def deconstration(sentS:str)->\"string\":\n",
        "  expanded_words = []\n",
        "  for word in sentS.split(\" \"):\n",
        "      expanded_words.append(contractions.fix(word))\n",
        "  return ' '.join(expanded_words)\n",
        "fileDict = dict()\n",
        "dict_f = dict()\n",
        "x = 0\n",
        "\n",
        "with open(\"test1.txt\") as file:\n",
        "    temp = []\n",
        "    for  f in file.readlines():\n",
        "        temp.append(f.replace(\"\\n\", ''))\n",
        "    temp.remove('')\n",
        "    fileDict = {i: deconstration(v) for i, v in enumerate(temp)}\n",
        "print(fileDict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'HC:Unsafe condition: Excavation access not provided', 1: 'HC:Unsafe environment: No access for is provided in the excavation', 2: 'HC:Unsafe condition: Entry and exit is not provided', 3: 'HC:Unsafe condition: No access for ingress/egress is provided in the excavation'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSZFEkgyP88M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c803e62d-e5df-4f21-d6c9-5473c21cc8a7"
      },
      "source": [
        "# https://nlpforhackers.io/wordnet-sentence-similarity/\n",
        "rom nltk import word_tokenize, pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet as wn\n",
        " \n",
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        "def tagged_to_synset(word, tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag is None:\n",
        "        return None\n",
        "    try:\n",
        "        return wn.synsets(word, wn_tag)[0]\n",
        "    except:\n",
        "        return None\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
        "    # Tokenize and tag\n",
        "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
        "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
        " \n",
        "    # Get the synsets for the tagged words\n",
        "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
        "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
        " \n",
        "    # Filter out the Nones\n",
        "    synsets1 = [ss for ss in synsets1 if ss]\n",
        "    synsets2 = [ss for ss in synsets2 if ss]\n",
        "    print(synsets1)\n",
        "    print(synsets2)\n",
        "    score, count = 0.0, 0\n",
        " \n",
        "    # For each word in the first sentence\n",
        "    for synset in synsets1:\n",
        "    # Get the similarity value of the most similar word in the other sentence\n",
        "      simlist = [synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None]\n",
        "      if not simlist:\n",
        "          continue;\n",
        "      best_score = max(simlist)\n",
        "\n",
        "      # Check that the similarity could have been computed\n",
        "      score += best_score\n",
        "      count += 1\n",
        "\n",
        "    if count == 0:\n",
        "      return 0\n",
        "\n",
        "    # Average the values\n",
        "    score /= count\n",
        "    return score\n",
        "\n",
        "def symmetric_sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
        "    return (sentence_similarity(sentence1, sentence2) + sentence_similarity(sentence2, sentence1)) / 2 \n",
        " \n",
        "sentences = [\n",
        "    \"HC:Unsafe condition: Excavation access not provided\",\n",
        "    \"HC:Unsafe environment: No access for is provided in the excavation\",\n",
        "    \"HC:Unsafe condition: Entry and exit is not provided\",\n",
        "    \"HC:Unsafe condition: No access for ingress/egress is provided in the excavation\",\n",
        "]\n",
        " \n",
        "focus_sentence = \"HC:Unsafe condition: Access provided in the excavation for ingress/egress\"\n",
        " \n",
        "for sentence in sentences:\n",
        "    print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, symmetric_sentence_similarity(focus_sentence, sentence)))\n",
        "    # print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, symmetric_sentence_similarity(sentence, focus_sentence)))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('excavation.n.01'), Synset('entree.n.02'), Synset('not.r.01'), Synset('supply.v.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('excavation.n.01'), Synset('entree.n.02'), Synset('not.r.01'), Synset('supply.v.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "Similarity(\"HC:Unsafe condition: Access provided in the excavation for ingress/egress\", \"HC:Unsafe condition: Excavation access not provided\") = 1.0\n",
            "\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('environment.n.01'), Synset('entree.n.02'), Synset('be.v.01'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('environment.n.01'), Synset('entree.n.02'), Synset('be.v.01'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "Similarity(\"HC:Unsafe condition: Access provided in the excavation for ingress/egress\", \"HC:Unsafe environment: No access for is provided in the excavation\") = 0.8069444444444445\n",
            "\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entry.n.01'), Synset('exit.n.01'), Synset('be.v.01'), Synset('not.r.01'), Synset('supply.v.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entry.n.01'), Synset('exit.n.01'), Synset('be.v.01'), Synset('not.r.01'), Synset('supply.v.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "Similarity(\"HC:Unsafe condition: Access provided in the excavation for ingress/egress\", \"HC:Unsafe condition: Entry and exit is not provided\") = 0.6111111111111112\n",
            "\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('be.v.01'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('be.v.01'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "[Synset('insecure.a.02'), Synset('condition.n.01'), Synset('entree.n.02'), Synset('supply.v.01'), Synset('excavation.n.01')]\n",
            "Similarity(\"HC:Unsafe condition: Access provided in the excavation for ingress/egress\", \"HC:Unsafe condition: No access for ingress/egress is provided in the excavation\") = 0.9444444444444444\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yywqlJ_nu3yT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}